<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Feature Selection</title>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/styles/github.min.css">
<script src="https://cdn.jsdelivr.net/combine/gh/highlightjs/cdn-release@11.6.0/build/highlight.min.js,npm/@xiee/utils/js/load-highlight.js" async></script>

<!-- MathJax scripts -->
<script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>

<style type="text/css">
body, td {
  font-family: sans-serif;
  background-color: white;
  font-size: 13px;
}
body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
}
tt, code, pre {
  font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}
a:visited { color: #80007f; }
pre, img { max-width: 100%; }
code {
  font-size: 92%;
  border: 1px solid #ccc;
}
code[class] { background-color: #F8F8F8; }
code.language-undefined { background-color: inherit; }
table {
  margin: auto;
  border-top: 1px solid #666;
  border-bottom: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color:#666;
  margin:0;
  padding-left: 1em;
  border-left: 0.5em #eee solid;
}
hr { border: 1px #ddd dashed; }

@media print {
  * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
  }
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  a, a:visited { text-decoration: underline; }
  hr {
    visibility: hidden;
    page-break-before: always;
  }
  pre, blockquote {
    padding-right: 1em;
    page-break-inside: avoid;
  }
  tr, img { page-break-inside: avoid; }
  img { max-width: 100% !important; }
  @page :left { margin: 15mm 20mm 15mm 10mm; }
  @page :right { margin: 15mm 10mm 15mm 20mm; }
  p, h2, h3 { orphans: 3; widows: 3; }
  h2, h3 { page-break-after: avoid; }
}
</style>



</head>

<body>
<pre><code class="language-{.r">library(dplyr); library(tidyr); library(purrr) # Data wrangling
library(ggplot2); library(stringr) # Plotting
library(tidyfit) # Model fitting

# Max model size
MODEL_SIZE &lt;- 10
</code></pre>
<p><code>tidyfit</code> packages several methods that can be used for feature selection. These include <strong>filter</strong>, <strong>wrapper</strong> and <strong>embedded</strong> algorithms. In this tutorial, we will use 3 algorithms from each of these broader categories, in order to select the top 10 best predictors of industrial production from a macroeconomic data set of monthly US economic indicators (FRED-MD).</p>
<p>The FRED-MD data set contains 134 variables characterizing the US macroeconomy with a monthly frequency and values (across all features) since the early 1990s. The data set is often used in academic research — primarily for the development of high-dimensional forecasting and nowcasting. All variables have conveniently been transformed to ensure stationarity. A description of the data as well as transformations can be found <a href="https://research.stlouisfed.org/econ/mccracken/fred-databases/">here</a>.</p>
<p>The data can be downloaded using the <code>fbi</code>-package in <code>R</code>.^[See <code>?fbi::fredmd</code>.] In addition to the variables included there, I augment the ISM manufacturing PMI data (6 features), which is no longer provided by FRED.</p>
<pre><code class="language-r"># Load the data
data &lt;- readRDS(&quot;FRED-MD.rds&quot;)
</code></pre>
<p>Let’s shift the target to generate a forecast, and drop missing values:</p>
<pre><code class="language-r">data &lt;- data %&gt;% 
  arrange(date) %&gt;% 
  # Shift the target by 1 month
  mutate(Target = lead(INDPRO)) %&gt;% 
  drop_na %&gt;% 
  select(-date)

data
#&gt; # A tibble: 363 × 135
#&gt;          RPI   W875RX1 DPCERA3M08…¹ CMRMTS…²  RETAILx   INDPRO  IPFPNSS  IPFINAL
#&gt;        &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
#&gt;  1  0.00165   0.000812     0.00182   0.00402 -0.00300  8.28e-3  8.51e-3  0.00899
#&gt;  2  0.00373   0.00275      0.000839  0.00664  0.00602  7.59e-3  7.43e-3  0.00760
#&gt;  3  0.00533   0.00549      0.00515  -0.00874  0.00547  3.22e-3  5.50e-3  0.00580
#&gt;  4  0.00417   0.00445      0.00270   0.0105   0.00280  5.39e-4 -2.44e-3 -0.00309
#&gt;  5 -0.000796 -0.00135      0.00334   0.0133   0.00708  8.91e-3  9.98e-3  0.0113 
#&gt;  6  0.00386   0.00381      0.00247  -0.0160   0.00324 -5.57e-3 -1.14e-3 -0.00119
#&gt;  7 -0.00383  -0.00495      0.00649   0.0138   0.00949  3.14e-3 -5.70e-4 -0.00190
#&gt;  8 -0.00492  -0.00580      0.00337   0.00404  0.00721  6.79e-3  7.75e-3  0.0101 
#&gt;  9  0.00218   0.00446      0.00171   0.00170  0.00221  4.03e-3  2.85e-3  0.00346
#&gt; 10  0.0358    0.0401       0.00656   0.0154   0.0122   1.58e-3  3.36e-3  0.00278
#&gt; # … with 353 more rows, 127 more variables: IPCONGD &lt;dbl&gt;, IPDCONGD &lt;dbl&gt;,
#&gt; #   IPNCONGD &lt;dbl&gt;, IPBUSEQ &lt;dbl&gt;, IPMAT &lt;dbl&gt;, IPDMAT &lt;dbl&gt;, IPNMAT &lt;dbl&gt;,
#&gt; #   IPMANSICS &lt;dbl&gt;, IPB51222S &lt;dbl&gt;, IPFUELS &lt;dbl&gt;, CUMFNS &lt;dbl&gt;, HWI &lt;dbl&gt;,
#&gt; #   HWIURATIO &lt;dbl&gt;, CLF16OV &lt;dbl&gt;, CE16OV &lt;dbl&gt;, UNRATE &lt;dbl&gt;, UEMPMEAN &lt;dbl&gt;,
#&gt; #   UEMPLT5 &lt;dbl&gt;, UEMP5TO14 &lt;dbl&gt;, UEMP15OV &lt;dbl&gt;, UEMP15T26 &lt;dbl&gt;,
#&gt; #   UEMP27OV &lt;dbl&gt;, CLAIMSx &lt;dbl&gt;, PAYEMS &lt;dbl&gt;, USGOOD &lt;dbl&gt;,
#&gt; #   CES1021000001 &lt;dbl&gt;, USCONS &lt;dbl&gt;, MANEMP &lt;dbl&gt;, DMANEMP &lt;dbl&gt;, …
</code></pre>
<h2>Feature selection algorithms</h2>
<p>We will fit each of the feature selection algorithms using the <code>regress</code> function in <code>tidyfit</code>, and will iteratively build a <code>tidyfit.models</code>-frame below.</p>
<h3>Filter methods</h3>
<p>Filter methods are model-agnostic and perform univariate comparisons between each feature and the target. They encompass the simplest (and typically fastest) group of algorithms. One of the most basic forms of feature selection uses <strong>Pearson’s correlation</strong> coefficient. As with all <code>tidyfit</code> methods, <code>m(&quot;cor&quot;)</code> fits the method. The actual correlation coefficients will be obtained using <code>coef()</code>:</p>
<pre><code class="language-r"># Correlation
algorithms_df &lt;- data %&gt;% 
  regress(Target ~ ., Correlation = m(&quot;cor&quot;))
</code></pre>
<p>The <strong>ReliefF</strong> algorithm is a popular nearest-neighbors-based approach to feature selection and can be implemented using <code>m(&quot;relief&quot;)</code>. The function will automatically use the regression version (RReliefF) when executed within a <code>regress</code> wrapper:</p>
<pre><code class="language-r"># RReliefF
algorithms_df &lt;- algorithms_df %&gt;% 
  bind_rows(
    data %&gt;% 
      regress(Target ~ ., RReliefF = m(&quot;relief&quot;))
  )
</code></pre>
<p>Under the hood, <code>m(&quot;relief&quot;)</code> is a wrapper for <code>CORElearn::attrEval</code>, which bundles a large number of selection algorithms, with <code>estimator = &quot;RReliefFequalK&quot;</code> as the default. This default can be overridden to employ any alternative feature selection algorithm, such as another nonparametric method called <strong>information gain</strong>. This method first requires the continuous target to be bucketized:</p>
<pre><code class="language-r"># Information Gain
algorithms_df &lt;- algorithms_df %&gt;% 
  bind_rows(
    data %&gt;% 
      # Split target into buckets
      mutate(Target = as.factor(ntile(Target, 10))) %&gt;% 
      regress(Target ~ ., 
              `Information Gain` = m(&quot;relief&quot;, estimator = &quot;InfGain&quot;))
  )
</code></pre>
<h3>Wrapper methods</h3>
<p>The next set of methods perform iterative feature selection. The methods fit a model in a sequential manner, eliminating or adding features based on some criterion of model fit or predictive accuracy. We begin with <strong>forward selection</strong>, performed using <code>m(&quot;subset&quot;)</code>. Here we can specify the target model size directly:</p>
<pre><code class="language-r"># Forward Selection
algorithms_df &lt;- algorithms_df %&gt;% 
  bind_rows(
    data %&gt;% 
      regress(Target ~ ., 
              `Forward Selection` = m(&quot;subset&quot;, method = &quot;forward&quot;, nvmax = MODEL_SIZE))
  )
</code></pre>
<p>The opposite of forward selection is <strong>backward elimination</strong>. This method is also implemented using the “subset” wrapper:</p>
<pre><code class="language-r"># Backward Elimination
algorithms_df &lt;- algorithms_df %&gt;% 
  bind_rows(
    data %&gt;% 
      regress(Target ~ ., 
              `Backward Elimination` = m(&quot;subset&quot;, method = &quot;backward&quot;, nvmax = MODEL_SIZE))
  )
</code></pre>
<p>The final sequential algorithm examined here is <strong>minimum redundancy, maximum relevance (MRMR)</strong>. The algorithm selects features based on the dual objective of maximizing the relevance for the target, while minimizing redundant information in the feature set. <code>m(&quot;mrmr&quot;)</code> is a wrapper for <code>mRMRe::mRMR.ensemble</code>:</p>
<pre><code class="language-r"># MRMR
algorithms_df &lt;- algorithms_df %&gt;% 
  bind_rows(
    data %&gt;% 
      regress(Target ~ ., MRMR = m(&quot;mrmr&quot;, feature_count = MODEL_SIZE))
  )
</code></pre>
<h3>Embedded methods</h3>
<p>The last group of feature selection algorithms, embedded methods, combine model selection and estimation into a single step — for instance, by forcing a subset of the parameter weights to be zero. The <strong>LASSO</strong> does this by introducing an \(L1\)-penalty on the parameters. Here we will use an expanding window grid search validation to determine the optimal penalty^[See <code>?rsample::rolling_origin</code> for details]:</p>
<pre><code class="language-r"># LASSO
algorithms_df &lt;- algorithms_df %&gt;% 
  bind_rows(
    data %&gt;% 
      regress(Target ~ ., 
              `LASSO` = m(&quot;lasso&quot;, pmax = MODEL_SIZE + 1),
              .cv = &quot;rolling_origin&quot;, 
              .cv_args = list(initial = 120, assess = 24, skip = 23)
              )
  )
</code></pre>
<p><strong>Bayesian model averaging</strong> takes a different approach, sampling a large number of models and using Bayes’ rule to compute a posterior inclusion probability for each feature. <code>m(&quot;bma&quot;)</code> is a wrapper for <code>BMS::bms</code>:</p>
<pre><code class="language-r"># BMA
algorithms_df &lt;- algorithms_df %&gt;% 
  bind_rows(
    data %&gt;% 
      regress(Target ~ ., 
              BMA = m(&quot;bma&quot;, burn = 10000, iter = 100000, 
                      mprior.size = MODEL_SIZE, mcmc = &quot;rev.jump&quot;))
  )
</code></pre>
<p>Last, but not least, <strong>Random Forests importance</strong> is a popular machine learning technique for model selection. We estimate a simple random forest using default settings, with <code>m(&quot;rf&quot;)</code>, which is a wrapper for the <code>randomForest</code>-package. Note that <code>importance = TRUE</code> by default, thus feature importances are computed and can be accessed using <code>coef</code>:</p>
<pre><code class="language-r"># Random Forest Importance
algorithms_df &lt;- algorithms_df %&gt;% 
  bind_rows(
    data %&gt;% 
      regress(Target ~ ., `RF Importance` = m(&quot;rf&quot;))
  )
</code></pre>
<h2>Extracting the top models</h2>
<p>All information needed to select the top 10 features for each algorithm can be obtained using <code>coef(algorithms_df)</code> and unnesting the additional information stored in <code>model_info</code>:</p>
<pre><code class="language-r">coef_df &lt;- coef(algorithms_df) %&gt;% 
  unnest(model_info)
</code></pre>
<p>Some algorithms return more than the maximum 10 variables. For instance, the filter methods (correlation, RReliefF and information gain) return a score for each feature. The below code chunk selects the top 10 features for each algorithm:</p>
<pre><code class="language-r">model_df &lt;- coef_df %&gt;% 
  # Always remove the intercept
  filter(term != &quot;(Intercept)&quot;) %&gt;% 
  
  mutate(selected = case_when(
    # Extract top 10 largest scores
    model %in% c(&quot;Correlation&quot;, &quot;RReliefF&quot;, &quot;Information Gain&quot;) ~ 
      rank(-abs(estimate)) &lt;= MODEL_SIZE,
    # BMA features are selected using the posterior inclusion probability
    model == &quot;BMA&quot; ~ rank(-pip) &lt;= MODEL_SIZE,
    # The RF importance is stored in a separate column (%IncMSE)
    model == &quot;RF Importance&quot; ~ rank(-`%IncMSE`) &lt;= MODEL_SIZE,
    # For all other methods keep all features
    TRUE ~ TRUE
  )) %&gt;% 
  
  # Keep only included terms
  filter(selected) %&gt;% 
  select(model, term)
</code></pre>
<p>Before examining the results, we will also add a <strong>domain expert</strong>. Here I simply add those features that are included in the US Conference Board Composite Leading Indicator and are available in our data set:</p>
<pre><code class="language-r">model_df &lt;- model_df %&gt;% 
  bind_rows(tibble(
    model = &quot;Domain Expert&quot;,
    term = c(&quot;NAPMNOI&quot;, &quot;ANDENOx&quot;, &quot;CLAIMSx&quot;, &quot;ACOGNO&quot;, 
             &quot;S&amp;P 500&quot;, &quot;T10YFFM&quot;, &quot;PERMIT&quot;, &quot;AWHMAN&quot;)
  ))
</code></pre>
<p>Now, let’s examine the models selected by each of the various algorithms:</p>
<pre><code class="language-{.r">model_df %&gt;% 
  # Add 'FALSE' entries, when a feature is not selected
  mutate(selected = TRUE) %&gt;% 
  spread(term, selected) %&gt;% 
  gather(&quot;term&quot;, &quot;selected&quot;, -model) %&gt;% 
  # Plotting color
  mutate(selected = ifelse(is.na(selected), &quot;white&quot;, &quot;darkblue&quot;)) %&gt;% 
  # Fix plotting order
  group_by(term) %&gt;% 
  mutate(selected_sum = sum(selected==&quot;darkblue&quot;)) %&gt;% 
  ungroup %&gt;% 
  arrange(desc(selected_sum)) %&gt;% 
  mutate(term = factor(term, levels = unique(term))) %&gt;% 
  mutate(model = factor(model, levels = unique(model_df$model))) %&gt;% 
  ggplot(aes(term, model)) +
  geom_tile(aes(fill = selected)) +
  theme_bw(8, &quot;Arial&quot;) +
  scale_fill_identity() +
  xlab(element_blank()) + ylab(element_blank()) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
</code></pre>
<img src="data:image/png;base64,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" alt="plot of chunk unnamed-chunk-16" style="display: block; margin: auto;" />
<p>There is quite a lot of disagreement between different feature selection algorithms. We can develop an understanding of the type of information selected by each algorithm by examining how well each feature set explains the target using the \(R2\)-statistic. To get a sense of the sample stability, we generate bootstrap samples and regress the target onto each model. Note that it is important to set <code>.force_cv = TRUE</code> below, since <code>m(&quot;lm&quot;)</code> does not have hyperparameters and thus ignores the <code>.cv</code> argument by default.</p>
<pre><code class="language-r">model_names &lt;- unique(model_df$model)

# Retrieve selected variables
selected_vars_list &lt;- model_names %&gt;% 
  map(function(mod) {
    model_df %&gt;% 
      filter(model == mod) %&gt;% 
      pull(term)
  })
names(selected_vars_list) &lt;- model_names

# Bootstrap resampling &amp; regression
boot_models_df &lt;- selected_vars_list %&gt;% 
  map_dfr(function(selected_vars) {
    data %&gt;% 
      select(all_of(c(&quot;Target&quot;, selected_vars))) %&gt;% 
      
      regress(Target ~ ., 
              # Use linear regression
              m(&quot;lm&quot;), 
              # Bootstrap settings (see ?rsample::bootstraps)
              .cv = &quot;bootstraps&quot;, .cv_args = list(times = 100), 
              # Make sure the results for each slice are returned
              .force_cv = T, .return_slices = T)
  }, .id = &quot;model&quot;)

# Finally, extract R2 from the model results
boot_df &lt;- boot_models_df %&gt;% 
  mutate(R2 = map_dbl(model_object, function(obj) summary(obj)$r.squared)) %&gt;% 
  select(model, R2)
</code></pre>
<pre><code class="language-{.r">boot_df %&gt;% 
  group_by(model) %&gt;% 
  mutate(upper = mean(R2) + 2 * sd(R2) / sqrt(n()),
         lower = mean(R2) - 2 * sd(R2) / sqrt(n())) %&gt;% 
  mutate(model = str_wrap(model, 10)) %&gt;% 
  mutate(model = factor(model, levels = str_wrap(unique(model_df$model), 10))) %&gt;% 
  ggplot(aes(model)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), linewidth = 0.25, width = 0.25) +
  theme_bw(8, &quot;Arial&quot;) +
  xlab(element_blank()) + ylab(&quot;R2 statistic&quot;)
</code></pre>
<img src="data:image/png;base64,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" alt="plot of chunk unnamed-chunk-18" style="display: block; margin: auto;" />
<p>The filter methods as well as RF Importance tend to explain a relatively small proportion of the response variation, while BMA and subset selection algorithms perform best.</p>


<script src="https://cdn.jsdelivr.net/combine/npm/@xiee/utils/js/center-img.min.js" async></script>
</body>

</html>

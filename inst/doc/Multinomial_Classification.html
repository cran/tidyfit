<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Multinomial Classification</title>
<style type="text/css">
/**
 * Prism.s theme ported from highlight.js's xcode style
 */
pre code {
  padding: 1em;
}
.token.comment {
  color: #007400;
}
.token.punctuation {
  color: #999;
}
.token.tag,
.token.selector {
  color: #aa0d91;
}
.token.boolean,
.token.number,
.token.constant,
.token.symbol {
  color: #1c00cf;
}
.token.property,
.token.attr-name,
.token.string,
.token.char,
.token.builtin {
  color: #c41a16;
}
.token.inserted {
  background-color: #ccffd8;
}
.token.deleted {
  background-color: #ffebe9;
}
.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #9a6e3a;
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #836c28;
}
.token.function,
.token.class-name {
  color: #DD4A68;
}
.token.regex,
.token.important,
.token.variable {
  color: #5c2699;
}
.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}
</style>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  box-sizing: border-box;
}
body, .footnotes, code { font-size: .9em; }
li li { font-size: .95em; }
*, *:before, *:after {
  box-sizing: inherit;
}
pre, img { max-width: 100%; }
pre, pre:hover {
  white-space: pre-wrap;
  word-break: break-all;
}
pre code {
  display: block;
  overflow-x: auto;
}
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre) > code, code[class] { background-color: #F8F8F8; }
code.language-undefined, pre > code:not([class]) {
  background-color: inherit;
  border: 1px solid #eee;
}
table {
  margin: auto;
  border-top: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color: #666;
  margin: 0;
  padding-left: 1em;
  border-left: 0.5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC .numbered li { list-style: none; }
#TOC .numbered { padding-left: 0; }
#TOC .numbered ul { padding-left: 1em; }
table, .body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.footnote-ref a::before { content: "["; }
.footnote-ref a::after { content: "]"; }
.footnotes::before {
  content: "";
  display: block;
  max-width: 20em;
}

@media print {
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  tr, img { page-break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  pre { white-space: pre; }
}
</style>
</head>
<body>
<div class="include-before">
</div>
<div class="frontmatter">
<div class="title"><h1>Multinomial Classification</h1></div>
<div class="author"><h2></h2></div>
<div class="date"><h3></h3></div>
</div>
<div class="body">
<pre><code class="language-r">library(dplyr); library(tidyr); library(purrr) # Data wrangling
library(ggplot2); library(stringr) # Plotting
library(tidyfit)   # Auto-ML modeling
</code></pre>
<p>Multinomial classification is possible in <code>tidyfit</code> using the methods powered by <code>glmnet</code>, <code>e1071</code> and <code>randomForest</code> (LASSO, Ridge, ElasticNet,  AdaLASSO, SVM and Random Forest). Currently, none of the other methods support multinomial classification.^[Feature selection methods such as <code>relief</code> or <code>chisq</code> can be used with multinomial response variables. I may also add support for multinomial classification with <code>mboost</code> in future.] When the response variable contains more than 2 classes, <code>classify</code> automatically uses a multinomial response for the above-mentioned methods.</p>
<p>Here’s an example using the built-in <code>iris</code> dataset:</p>
<pre><code class="language-r">data(&quot;iris&quot;)

# For reproducibility
set.seed(42)
ix_tst &lt;- sample(1:nrow(iris), round(nrow(iris)*0.2))

data_trn &lt;- iris[-ix_tst,]
data_tst &lt;- iris[ix_tst,]

as_tibble(iris)
#&gt; # A tibble: 150 × 5
#&gt;    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#&gt;           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  
#&gt;  1          5.1         3.5          1.4         0.2 setosa 
#&gt;  2          4.9         3            1.4         0.2 setosa 
#&gt;  3          4.7         3.2          1.3         0.2 setosa 
#&gt;  4          4.6         3.1          1.5         0.2 setosa 
#&gt;  5          5           3.6          1.4         0.2 setosa 
#&gt;  6          5.4         3.9          1.7         0.4 setosa 
#&gt;  7          4.6         3.4          1.4         0.3 setosa 
#&gt;  8          5           3.4          1.5         0.2 setosa 
#&gt;  9          4.4         2.9          1.4         0.2 setosa 
#&gt; 10          4.9         3.1          1.5         0.1 setosa 
#&gt; # ℹ 140 more rows
</code></pre>
<h2 id="penalized-classification-algorithms-to-predict-species">Penalized classification algorithms to predict <code>Species</code></h2>
<p>The code chunk below fits the above mentioned algorithms on the training split, using a 10-fold cross validation to select optimal penalties. We then obtain out-of-sample predictions using <code>predict</code>. Unlike binomial classification, the <code>fit</code> and <code>pred</code> objects contain a <code>class</code> column with separate coefficients and predictions for each class. The predictions sum to one across classes:</p>
<pre><code class="language-r">fit &lt;- data_trn %&gt;% 
  classify(Species ~ ., 
           LASSO = m(&quot;lasso&quot;), 
           Ridge = m(&quot;ridge&quot;), 
           ElasticNet = m(&quot;enet&quot;), 
           AdaLASSO = m(&quot;adalasso&quot;),
           SVM = m(&quot;svm&quot;),
           `Random Forest` = m(&quot;rf&quot;),
           `Least Squares` = m(&quot;ridge&quot;, lambda = 1e-5), 
           .cv = &quot;vfold_cv&quot;)

pred &lt;- fit %&gt;% 
  predict(data_tst)
</code></pre>
<p>Note that we can add unregularized least squares estimates by setting <code>lambda = 0</code> (or very close to zero).</p>
<p>Next, we can use <code>yardstick</code> to calculate the log loss accuracy metric and compare the performance of the different models:</p>
<pre><code class="language-r">metrics &lt;- pred %&gt;% 
  group_by(model, class) %&gt;% 
  mutate(row_n = row_number()) %&gt;% 
  spread(class, prediction) %&gt;% 
  group_by(model) %&gt;% 
  yardstick::mn_log_loss(truth, setosa:virginica)

metrics %&gt;% 
  mutate(model = str_wrap(model, 11)) %&gt;% 
  ggplot(aes(model, .estimate)) +
  geom_col(fill = &quot;darkblue&quot;) +
  theme_bw() +
  theme(axis.title.x = element_blank())
</code></pre>
<img src="data:image/png;base64,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" alt="plot of chunk unnamed-chunk-5" style="display: block; margin: auto;" />
<p>The least squares estimate performs poorest, while the random forest (nonlinear) and the support vector machine (SVM) achieve the best results. The SVM is estimated with a linear kernel by default (use <code>kernel = &lt;chosen_kernel&gt;</code> to use a different kernel).</p>
</div>
<div class="include-after">
</div>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</body>
</html>

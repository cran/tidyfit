<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Multinomial Classification</title>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/styles/github.min.css">
<script src="https://cdn.jsdelivr.net/combine/gh/highlightjs/cdn-release@11.6.0/build/highlight.min.js,npm/@xiee/utils/js/load-highlight.js" async></script>



<style type="text/css">
body, td {
  font-family: sans-serif;
  background-color: white;
  font-size: 13px;
}
body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
}
tt, code, pre {
  font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}
a:visited { color: #80007f; }
pre, img { max-width: 100%; }
code {
  font-size: 92%;
  border: 1px solid #ccc;
}
code[class] { background-color: #F8F8F8; }
code.language-undefined { background-color: inherit; }
table {
  margin: auto;
  border-top: 1px solid #666;
  border-bottom: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color:#666;
  margin:0;
  padding-left: 1em;
  border-left: 0.5em #eee solid;
}
hr { border: 1px #ddd dashed; }

@media print {
  * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
  }
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  a, a:visited { text-decoration: underline; }
  hr {
    visibility: hidden;
    page-break-before: always;
  }
  pre, blockquote {
    padding-right: 1em;
    page-break-inside: avoid;
  }
  tr, img { page-break-inside: avoid; }
  img { max-width: 100% !important; }
  @page :left { margin: 15mm 20mm 15mm 10mm; }
  @page :right { margin: 15mm 10mm 15mm 20mm; }
  p, h2, h3 { orphans: 3; widows: 3; }
  h2, h3 { page-break-after: avoid; }
}
</style>



</head>

<body>
<pre><code class="language-r">library(dplyr); library(tidyr); library(purrr) # Data wrangling
library(ggplot2); library(stringr) # Plotting
library(tidyfit)   # Auto-ML modeling
</code></pre>
<p>Multinomial classification is possible in <code>tidyfit</code> using the methods powered by <code>glmnet</code>, <code>e1071</code> and <code>randomForest</code> (LASSO, Ridge, ElasticNet,  AdaLASSO, SVM and Random Forest). Currently, none of the other methods support multinomial classification.^[Feature selection methods such as <code>relief</code> or <code>chisq</code> can be used with multinomial response variables. I may also add support for multinomial classification with <code>mboost</code> in future.] When the response variable contains more than 2 classes, <code>classify</code> automatically uses a multinomial response for the above-mentioned methods.</p>
<p>Here’s an example using the built-in <code>iris</code> dataset:</p>
<pre><code class="language-r">data(&quot;iris&quot;)

# For reproducibility
set.seed(42)
ix_tst &lt;- sample(1:nrow(iris), round(nrow(iris)*0.2))

data_trn &lt;- iris[-ix_tst,]
data_tst &lt;- iris[ix_tst,]

as_tibble(iris)
#&gt; # A tibble: 150 × 5
#&gt;    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#&gt;           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  
#&gt;  1          5.1         3.5          1.4         0.2 setosa 
#&gt;  2          4.9         3            1.4         0.2 setosa 
#&gt;  3          4.7         3.2          1.3         0.2 setosa 
#&gt;  4          4.6         3.1          1.5         0.2 setosa 
#&gt;  5          5           3.6          1.4         0.2 setosa 
#&gt;  6          5.4         3.9          1.7         0.4 setosa 
#&gt;  7          4.6         3.4          1.4         0.3 setosa 
#&gt;  8          5           3.4          1.5         0.2 setosa 
#&gt;  9          4.4         2.9          1.4         0.2 setosa 
#&gt; 10          4.9         3.1          1.5         0.1 setosa 
#&gt; # … with 140 more rows
</code></pre>
<h2>Penalized classification algorithms to predict <code>Species</code></h2>
<p>The code chunk below fits the above mentioned algorithms on the training split, using a 10-fold cross validation to select optimal penalties. We then obtain out-of-sample predictions using <code>predict</code>. Unlike binomial classification, the <code>fit</code> and <code>pred</code> objects contain a <code>class</code> column with separate coefficients and predictions for each class. The predictions sum to one across classes:</p>
<pre><code class="language-r">fit &lt;- data_trn %&gt;% 
  classify(Species ~ ., 
           LASSO = m(&quot;lasso&quot;), 
           Ridge = m(&quot;ridge&quot;), 
           ElasticNet = m(&quot;enet&quot;), 
           AdaLASSO = m(&quot;adalasso&quot;),
           SVM = m(&quot;svm&quot;),
           `Random Forest` = m(&quot;rf&quot;),
           `Least Squares` = m(&quot;ridge&quot;, lambda = 1e-5), 
           .cv = &quot;vfold_cv&quot;)

pred &lt;- fit %&gt;% 
  predict(data_tst)
</code></pre>
<p>Note that we can add unregularized least squares estimates by setting <code>lambda = 0</code> (or very close to zero).</p>
<p>Next, we can use <code>yardstick</code> to calculate the log loss accuracy metric and compare the performance of the different models:</p>
<pre><code class="language-r">metrics &lt;- pred %&gt;% 
  group_by(model, class) %&gt;% 
  mutate(row_n = row_number()) %&gt;% 
  spread(class, prediction) %&gt;% 
  group_by(model) %&gt;% 
  yardstick::mn_log_loss(truth, setosa:virginica)

metrics %&gt;% 
  mutate(model = str_wrap(model, 11)) %&gt;% 
  ggplot(aes(model, .estimate)) +
  geom_col(fill = &quot;darkblue&quot;) +
  theme_bw() +
  theme(axis.title.x = element_blank())
</code></pre>
<img src="data:image/png;base64,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" alt="plot of chunk unnamed-chunk-5" style="display: block; margin: auto;" />
<p>The least squares estimate performs poorest, while the random forest (nonlinear) and the support vector machine (SVM) achieve the best results. The SVM is estimated with a linear kernel by default (use <code>kernel = &lt;chosen_kernel&gt;</code> to use a different kernel).</p>


<script src="https://cdn.jsdelivr.net/combine/npm/@xiee/utils/js/center-img.min.js" async></script>
</body>

</html>
